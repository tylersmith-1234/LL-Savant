{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using existing login session\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import configparser\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from constants_funcs import get_session, int_or_float_or_str\n",
    "import json\n",
    "\n",
    "with open('constants.json', 'r') as d:\n",
    "    constants = json.load(d)\n",
    "    \n",
    "sess = get_session()\n",
    "sess\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "search_term = '' # if you want to find specific players by name change this search term\n",
    "all_id_url = constants['LLHEADER'] + '/backend-search.php?term=' + search_term\n",
    "all_id_url\n",
    "\n",
    "ids_html = sess.get(all_id_url)\n",
    "html_text = ids_html.text\n",
    "soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "rows = soup.find_all('a')\n",
    "dic = dict() # CAVEAT: ONLY FINDS CURRENTLY ACTIVE PLAYERS\n",
    "for row in rows:\n",
    "    id = int(row['href'].split('?')[1])\n",
    "    name = row.find('span').text\n",
    "    loc = row.find('i').text\n",
    "    dic[id] = {'name': name, 'loc': loc}\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def get_children(id):\n",
    "    referrals_url = constants['LLHEADER'] + f'/profiles.php?{str(id)}&5'\n",
    "    print(referrals_url)\n",
    "    main_data = sess.get(referrals_url)\n",
    "    html_text = main_data.text\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    header = soup.find(text=\"Direct Referrals \").next_element.next_element.next_element.next_element\n",
    "    return header\n",
    "\n",
    "\n",
    "def init_profile(id):\n",
    "    url = constants['LLHEADER'] + f'/profiles.php?{str(id)}'\n",
    "    main_data = sess.get(url)\n",
    "    html_text = main_data.text\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    ret = dict()\n",
    "\n",
    "    if 'This player is inactive.' in soup.text:\n",
    "        ret['status'] = 'inactive'\n",
    "        ret['name'] = soup.find('div', attrs={'class': 'namediv'}).text.strip()\n",
    "    else:\n",
    "        box = soup.find('div', attrs={'class': 'topcont'})\n",
    "        if '\\t\\tPassed away' in box.text:\n",
    "            ret['status'] = 'deceased'\n",
    "        else:\n",
    "            ret['status'] = box['class'][-1]\n",
    "        ret['branch'] = soup.findAll('div', attrs={'class': 'demog_row'})[1].text.strip().split()[1]\n",
    "        genloc = soup.findAll('div', attrs={'class': 'demog_row'})[0].text.strip().split('\\n\\n')\n",
    "        ret['gender'] = genloc[0].strip().split(': ')[-1]\n",
    "        ret['location'] = genloc[1].strip().split(': ')[-1]\n",
    "        ret['referrer'] = int(soup.findAll('div', attrs={'class': 'demog'})[4].find('a')['href'].split('?')[-1])\n",
    "        ret['name'] = soup.find('h1', attrs={'class': 'namecss'}).text\n",
    "        ret['league'] = soup.find('div', attrs={'class': 'leaguelogodiv'}).text.strip()\n",
    "        record_rows = soup.find('table', attrs={'summary': 'Data table for career record'}).findAll('tr')[1:]\n",
    "        record_dic = dict()\n",
    "        for row in record_rows:\n",
    "            level, record, _ = row.text.strip().split('\\n')\n",
    "            level = level.split(' ')[-1]\n",
    "            record_dic[level] = {\n",
    "                'W': int(record.split('-')[0]),\n",
    "                'L': int(record.split('-')[1]),\n",
    "                'T': int(record.split('-')[2]),\n",
    "            }\n",
    "        ret['record'] = pd.DataFrame.from_dict(record_dic, orient='index')\n",
    "        data_table = soup.find('table', attrs={'class': 'std sortable this_sea std_bord'})\n",
    "        rows = data_table.findAll('tr')\n",
    "        headers = ['Category', 'Career', 'Career Pct', 'Lg Pct', 'Recent Szn', 'Szn Pct', 'Szn Lg Pct', 'Rundle Pct']\n",
    "        tbl = [[int_or_float_or_str(x.text) for x in row.findAll('td')] for row in rows[1:]]\n",
    "        df = pd.DataFrame(tbl, columns=headers)\n",
    "        df['Career TCA'] = df.apply(lambda row: int(row['Career'].split('-')[0]), axis=1)\n",
    "        df['Career Q'] = df.apply(lambda row: int(row['Career'].split('-')[1]), axis=1)\n",
    "        df['Szn TCA'] = df.apply(lambda row: int(row['Recent Szn'].split('-')[0]), axis=1)\n",
    "        df['Szn Q'] = df.apply(lambda row: int(row['Recent Szn'].split('-')[1]), axis=1)\n",
    "        df.drop(['Career', 'Career Pct', 'Recent Szn', 'Szn Pct'], axis=1, inplace=True)\n",
    "        new_headers = ['Category', 'Career TCA', 'Career Q', 'Lg Pct', 'Szn TCA', 'Szn Q', 'Szn Lg Pct', 'Rundle Pct']\n",
    "        df = df[new_headers]\n",
    "        ret['Q_data'] = df\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = pd.read_csv('./FULL_QUESTION_HISTORY.csv')\n",
    "\n",
    "def build_prediction(profileno,seas,mday):\n",
    "    myprofile = init_profile(profileno)\n",
    "    tca = myprofile['Q_data']['Career TCA']\n",
    "    q = myprofile['Q_data']['Career Q']\n",
    "    pct = tca/q\n",
    "    q_cat = myprofile['Q_data']['Category']\n",
    "    cat_pct = dict()\n",
    "    for x in range(len(q_cat)):\n",
    "        cat_pct[q_cat[x]] = float(pct[x])\n",
    "    \n",
    "    md = questions[(questions.season == seas)&(questions.matchday == mday)].reset_index()\n",
    "    ex = md.apply(lambda row: row['defense'] * cat_pct[row['category']], axis=1)\n",
    "    xPTS = sum(ex)\n",
    "    xCA = sum([cat_pct[cat] for cat in md['category']])\n",
    "    \n",
    "    daily_catpct = [cat_pct[cat] for cat in md['category']]\n",
    "    \n",
    "    md['cat_pct'] = daily_catpct\n",
    "    \n",
    "    md.sort_values(by=['cat_pct'], inplace=True)\n",
    "    md['ideal_d'] = [3,2,2,1,1,0]\n",
    "    \n",
    "    ex = md.apply(lambda row: row['ideal_d'] * cat_pct[row['category']], axis=1)\n",
    "    xPTS_ideal = sum(ex)\n",
    "    \n",
    "    md['worst_d'] = [0,1,1,2,2,3]\n",
    "    \n",
    "    ex = md.apply(lambda row: row['worst_d'] * cat_pct[row['category']], axis=1)\n",
    "    xPTS_worst = sum(ex)\n",
    "    \n",
    "    return xCA, xPTS, xPTS_ideal, xPTS_worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.9834430897939948, 4.403851190522072, 3.9532883259298357, 4.997040943452148)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_prediction(16642,92,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
